{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "62dmZ0B1hk-F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Memuat dataset\n",
        "file_path ='[Dataset]_(Viral_Konten_MedSos).xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Mengubah target menjadi kategori (misalnya, viral jika shares > 1400)\n",
        "threshold = 1400\n",
        "data['viral'] = np.where(data[' shares'] > threshold, 1, 0)\n",
        "\n",
        "# Memisahkan fitur dan target\n",
        "X = data.drop(columns=['url', ' shares', 'viral'])\n",
        "y = data['viral']\n",
        "\n",
        "# Membagi data menjadi set pelatihan dan pengujian\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalisasi fitur numerik\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Melatih model Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Memprediksi pada set pengujian\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Mengevaluasi performa model dengan confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-vDyoNHRq0i",
        "outputId": "f98daa7d-76a1-40f2-c9c2-d35fed752a01"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[2723 1352]\n",
            " [1295 2559]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.67      0.67      4075\n",
            "           1       0.65      0.66      0.66      3854\n",
            "\n",
            "    accuracy                           0.67      7929\n",
            "   macro avg       0.67      0.67      0.67      7929\n",
            "weighted avg       0.67      0.67      0.67      7929\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned dataset\n",
        "cleaned_file_path = '[Dataset]_(Viral_Konten_MedSos).xlsx'\n",
        "df = pd.read_excel(cleaned_file_path)\n",
        "\n",
        "# Print column names to verify the target column name\n",
        "print(\"Columns in the dataset:\", df.columns)\n",
        "\n",
        "# Display the first few rows of the dataframe to understand its structure\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVDs1A4-Nl90",
        "outputId": "394eaa20-b2d3-4ed0-e510-04a70b481588"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the dataset: Index(['url', ' timedelta', ' n_tokens_title', ' n_tokens_content',\n",
            "       ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
            "       ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',\n",
            "       ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',\n",
            "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
            "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
            "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',\n",
            "       ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',\n",
            "       ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',\n",
            "       ' self_reference_max_shares', ' self_reference_avg_sharess',\n",
            "       ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',\n",
            "       ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',\n",
            "       ' weekday_is_sunday', ' is_weekend', ' LDA_00', ' LDA_01', ' LDA_02',\n",
            "       ' LDA_03', ' LDA_04', ' global_subjectivity',\n",
            "       ' global_sentiment_polarity', ' global_rate_positive_words',\n",
            "       ' global_rate_negative_words', ' rate_positive_words',\n",
            "       ' rate_negative_words', ' avg_positive_polarity',\n",
            "       ' min_positive_polarity', ' max_positive_polarity',\n",
            "       ' avg_negative_polarity', ' min_negative_polarity',\n",
            "       ' max_negative_polarity', ' title_subjectivity',\n",
            "       ' title_sentiment_polarity', ' abs_title_subjectivity',\n",
            "       ' abs_title_sentiment_polarity', ' shares'],\n",
            "      dtype='object')\n",
            "                                                 url   timedelta  \\\n",
            "0  http://mashable.com/2013/01/07/amazon-instant-...         731   \n",
            "1  http://mashable.com/2013/01/07/ap-samsung-spon...         731   \n",
            "2  http://mashable.com/2013/01/07/apple-40-billio...         731   \n",
            "3  http://mashable.com/2013/01/07/astronaut-notre...         731   \n",
            "4   http://mashable.com/2013/01/07/att-u-verse-apps/         731   \n",
            "\n",
            "    n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
            "0               12                219          0.663594                1.0   \n",
            "1                9                255          0.604743                1.0   \n",
            "2                9                211          0.575130                1.0   \n",
            "3                9                531          0.503788                1.0   \n",
            "4               13               1072          0.415646                1.0   \n",
            "\n",
            "    n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs  ...  \\\n",
            "0                   0.815385           4                2          1  ...   \n",
            "1                   0.791946           3                1          1  ...   \n",
            "2                   0.663866           3                1          1  ...   \n",
            "3                   0.665635           9                0          1  ...   \n",
            "4                   0.540890          19               19         20  ...   \n",
            "\n",
            "    min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
            "0                0.100000                     0.7               -0.350000   \n",
            "1                0.033333                     0.7               -0.118750   \n",
            "2                0.100000                     1.0               -0.466667   \n",
            "3                0.136364                     0.8               -0.369697   \n",
            "4                0.033333                     1.0               -0.220192   \n",
            "\n",
            "    min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
            "0                  -0.600               -0.200000             0.500000   \n",
            "1                  -0.125               -0.100000             0.000000   \n",
            "2                  -0.800               -0.133333             0.000000   \n",
            "3                  -0.600               -0.166667             0.000000   \n",
            "4                  -0.500               -0.050000             0.454545   \n",
            "\n",
            "    title_sentiment_polarity   abs_title_subjectivity  \\\n",
            "0                  -0.187500                 0.000000   \n",
            "1                   0.000000                 0.500000   \n",
            "2                   0.000000                 0.500000   \n",
            "3                   0.000000                 0.500000   \n",
            "4                   0.136364                 0.045455   \n",
            "\n",
            "    abs_title_sentiment_polarity   shares  \n",
            "0                       0.187500      593  \n",
            "1                       0.000000      711  \n",
            "2                       0.000000     1500  \n",
            "3                       0.000000     1200  \n",
            "4                       0.136364      505  \n",
            "\n",
            "[5 rows x 61 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '[Dataset]_(Viral_Konten_MedSos).xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Data cleaning\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop any rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Data normalization\n",
        "scaler = StandardScaler()\n",
        "df[['timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens', 'n_non_stop_words',\n",
        "'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos',\n",
        "'average_token_length', 'num_keywords', 'kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
        "'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'kw_avg_avg',\n",
        "'elf_reference_min_shares', 'elf_reference_max_shares', 'elf_reference_avg_sharess',\n",
        "'global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words',\n",
        "'global_rate_negative_words', 'rate_positive_words', 'rate_negative_words',\n",
        "'avg_positive_polarity', 'in_positive_polarity', 'ax_positive_polarity',\n",
        "'avg_negative_polarity', 'in_negative_polarity', 'ax_negative_polarity',\n",
        "'title_subjectivity', 'title_sentiment_polarity', 'abs_title_subjectivity',\n",
        "'abs_title_sentiment_polarity']] = scaler.fit_transform(df[[' timedelta', ' n_tokens_title',' n_tokens_content',' n_unique_tokens',' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
        "' num_hrefs', ' num_self_hrefs', ' num_imgs',' num_videos', ' average_token_length',' num_keywords', ' kw_min_min', ' kw_max_min',' kw_avg_min', ' kw_min_max', ' kw_max_max',\n",
        "' kw_avg_max', ' kw_min_avg', ' kw_max_avg',' kw_avg_avg', ' self_reference_min_shares',' self_reference_max_shares',' self_reference_avg_sharess',' global_subjectivity',' global_sentiment_polarity',' global_rate_positive_words',' global_rate_negative_words',' rate_positive_words', ' rate_negative_words',' avg_positive_polarity', ' min_positive_polarity',' max_positive_polarity', ' avg_negative_polarity',' min_negative_polarity', ' max_negative_polarity',' title_subjectivity', ' title_sentiment_polarity',' abs_title_subjectivity', ' abs_title_sentiment_polarity']])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = df.drop([' shares', 'url'], axis=1)\n",
        "y = df[' shares']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n",
        "print(f'R2 Score: {r2:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hmGPaoi5huV",
        "outputId": "2cd30e9c-7f64-4f52-87ec-26a41147662f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "url                              0\n",
            " timedelta                       0\n",
            " n_tokens_title                  0\n",
            " n_tokens_content                0\n",
            " n_unique_tokens                 0\n",
            "                                ..\n",
            " title_subjectivity              0\n",
            " title_sentiment_polarity        0\n",
            " abs_title_subjectivity          0\n",
            " abs_title_sentiment_polarity    0\n",
            " shares                          0\n",
            "Length: 61, dtype: int64\n",
            "Mean Squared Error: 117482657.59\n",
            "R2 Score: 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_column_name = 'shares'"
      ],
      "metadata": {
        "id": "slMHJiCiqCIj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_column_name = 'shares'\n",
        "\n",
        "# Ensure the column exists in the dataframe\n",
        "if correct_column_name in df.columns:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(df[correct_column_name], bins=50, kde=True)\n",
        "    plt.title('Distribution of Shares')\n",
        "    plt.xlabel('Number of Shares')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"Column '{correct_column_name}' does not exist in the dataframe.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj--fOtVqGs2",
        "outputId": "db13af94-557c-4dcb-e284-520c3c9480d5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'shares' does not exist in the dataframe.\n"
          ]
        }
      ]
    }
  ]
}